{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c332cf78-5fea-4db2-ba28-bbc14c115e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "import turingpoint.gymnasium_utils as tp_gym_utils\n",
    "import turingpoint.sb3_utils as tp_sb3_utils\n",
    "import turingpoint.utils as tp_utils\n",
    "import turingpoint as tp\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05429425-3074-46aa-bd11-7337722b1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext mermaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b1065e-2d64-4e45-a35b-3c5cedc7a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1a57a1-a17d-467c-b2ca-d78d0d4d97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, num_episodes: int) -> float:\n",
    "\n",
    "  rewards_collector = tp_utils.Collector(['reward'])\n",
    "\n",
    "  def get_participants():\n",
    "    yield functools.partial(tp_gym_utils.call_reset, env=env)\n",
    "    yield from itertools.cycle([\n",
    "        functools.partial(tp_sb3_utils.call_predict, agent=agent, deterministic=True),\n",
    "        functools.partial(tp_gym_utils.call_step, env=env),\n",
    "        rewards_collector,\n",
    "        tp_gym_utils.check_done\n",
    "    ]) \n",
    "\n",
    "  evaluate_assembly = tp.Assembly(get_participants)\n",
    "\n",
    "  for _ in range(num_episodes):\n",
    "    _ = evaluate_assembly.launch()\n",
    "    # Note that we don't clear the rewards in 'rewards_collector', and so we continue to collect.\n",
    "\n",
    "  total_reward = sum(x['reward'] for x in rewards_collector.get_entries())\n",
    "\n",
    "  return total_reward / num_episodes\n",
    "\n",
    "\n",
    "def train(agent, total_timesteps):\n",
    "  agent.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "  # The agent here learns from its internal Gym environment.\n",
    "  # We could use a loop with participants also for training, yet this is not shown here.\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "  random.seed(1)\n",
    "  np.random.seed(1)\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  env = gym.make(env_name)\n",
    "\n",
    "  env.reset(seed=1)\n",
    "\n",
    "  agent = PPO(MlpPolicy, env, verbose=0) # use verbose=1 for debugging\n",
    "\n",
    "  mean_reward_before_train = evaluate(env, agent, 100)\n",
    "  print(\"before training\")\n",
    "  print(f'{mean_reward_before_train=}')\n",
    "\n",
    "  train(agent, total_timesteps=1_000)\n",
    "\n",
    "  mean_reward_after_train = evaluate(env, agent, 100)\n",
    "  print(\"after training\")\n",
    "  print(f'{mean_reward_after_train=}')\n",
    "\n",
    "  agent.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6101a3-4a5b-4ff5-bad8-0768a830bcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8281820ab674c298137e699a6e73beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "mean_reward_before_train=9.11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training\n",
      "mean_reward_after_train=154.38\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a241bc-f120-4b28-8115-13caf41b0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f805b83-7539-4afb-aaf2-c31487bed6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO.load(\"ppo_cartpole\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a4c4c5-aceb-42f0-8fae-4a773bc6e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in trange(10):\n",
    "    obs, info = env.reset()\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action, _state = agent.predict(obs)  # agent policy that uses the observation and info\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "        episode_over = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8584bd-1e75-4ec8-934e-c22a45729a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d4e23-ca40-4832-9375-1981fe51efb9",
   "metadata": {},
   "source": [
    "# How can we learn and What to learn?\n",
    "# How shall we use what we learn, to make smart decisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a8c4b-a2cf-4d96-81f0-23c66b7020cc",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Agent] -->|Takes action| B[Environment]\n",
    "    B -->|Returns new state and reward| A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51536c4-c2a8-4624-9252-f4453485e8ca",
   "metadata": {},
   "source": [
    "## Idea: learn a Policy\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start(( )) -->|Observation| A[Policy]\n",
    "    A -->|Action| End(( ))\n",
    "```\n",
    "\n",
    "### Example: *Given the board and castling rights, what is the best Chess move?*\n",
    "### Example (continues): *Given the current sensor readings, what angle should the steering wheel be in?*\n",
    "###\n",
    "### Can also be **non-deterministic**: for example with discrete set of actions\n",
    "### (In the drawing below Softmax activation is assumed. Alternatively, replace \"prob.\" with \"logits\" and go from there).\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start(( )) -->|Observation| A[Policy]\n",
    "    A -->|\"Action 1 (prob.)\"| End1(( ))\n",
    "    A -->|\"Action 2 (prob.)\"| End2(( ))\n",
    "    A -->|\"...\"| End(( ))\n",
    "    A -->|\"Action n (prob.)\"| Endn(( ))\n",
    "```\n",
    "\n",
    "[REINFORCE Turingpoing example (cart-pole)](https://github.com/zbenmo/turingpoint/blob/main/examples/cart-pole-gym-torch-reinforce.py)\n",
    "\n",
    "### But also for continues actions (assumed to be Gaussian distributed):\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start(( )) -->|Observation| A[Policy]\n",
    "    A -->|\"Action μ\"| End1(( ))\n",
    "    A -->|\"Action σ\"| End2(( ))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a5820-3171-42ae-8de1-2e8297478add",
   "metadata": {},
   "source": [
    "## Idea: learn a State-Value Function\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start(( )) -->|Observation| A[Value]\n",
    "    A -->|\"Expected Total (discounted) Reward\"| End(( ))\n",
    "```\n",
    "\n",
    "### Example: given the board and all possible moves, evaluate which of those moves leads to a board with the highest total reward?\n",
    "### (Pay attention: Unless we've just won, it is now the other player's turn).\n",
    "### Note: A value of a (Chess) state depends very much on the player:\n",
    "### Myself against Magnus Carlsen or against Gukesh Dommaraju, probably -1 for almost any game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3168e3-b98a-42ae-8d58-e5ed763a4e16",
   "metadata": {},
   "source": [
    "## Idea: learn a Action-Value Function (Q-Learning)\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start1(( )) -->|Observation| A[Q]\n",
    "    Start2(( )) -->|Action| A\n",
    "    A -->|\"Expected Total (discounted) Reward\"| End(( ))\n",
    "```\n",
    "\n",
    "### Example: Compare \"Nope\", \"Left\", \"Right\", \"Fire\". Which leads to best total reward?\n",
    "\n",
    "### When the action space is descrete, it may make sense to have multiple concurrent \"heads\" (note: no Softmax):\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start1(( )) -->|Observation| A[Q]\n",
    "    A -->|\"Expected Total (discounted) Reward (for action 1)\"| End1(( ))\n",
    "    A -->|\"Expected Total (discounted) Reward (for action 2)\"| End2(( ))\n",
    "    A -->|\"...\"| End3(( ))\n",
    "    A -->|\"Expected Total (discounted) Reward (for action n)\"| Endn(( ))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0b098-2b3a-41d9-ab25-682403d42f50",
   "metadata": {},
   "source": [
    "## Idea: learn an Actor and a Critic\n",
    "\n",
    "### Ex. a Policy and a State-Value Function\n",
    "\n",
    "### Why is it any better than just learning the State-Value Function? We're going to choose the maximum anyhow..\n",
    "### Well, if we train (the Actor) using gradient decent, it helps if we calculate, for the sake of the loss, the \"Advantage\" of an action in a state. So it helps to center the values around zero.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start1(( )) -->|Observation| A[Critic]\n",
    "    A -->|\"Expected Total (discounted) Reward\"| C[Calc. Advantages]\n",
    "    Start2(( )) -->|Observation| B[Actor]\n",
    "    B -->|\"Expected Total (discounted) Reward\"| End1(( )) --> C\n",
    "    B -->|\"Expected Total (discounted) Reward\"| End2(( )) --> C\n",
    "    B -->|\"...\"| End3(( )) --> C\n",
    "    B -->|\"Expected Total (discounted) Reward\"| Endn(( )) --> C\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7489cab-5786-4f2b-881c-f1b1eb16a9ed",
   "metadata": {},
   "source": [
    "## Idea: learn a State-Value Function but also a Model of the Environment's transitions\n",
    "## (What happens if we take action _a_ at state _s_?)\n",
    "## Use the Model, for example for Planning. \n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Start1(( )) -->|Observation| A[Model]\n",
    "    Start2(( )) -->|\"Action 1\"| A\n",
    "    A -->|Expected next Observation| B[Value] --> |\"Expected Total (discounted) Reward (Action 1)\"| End1(( ))\n",
    "    Start1 -->|Observation| A2[Model]\n",
    "    Start4(( )) -->|\"Action 2\"| A2\n",
    "    A2 -->|Expected next Observation| B2[Value] --> |\"Expected Total (discounted) Reward (Action 2)\"| End2(( ))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a9d4f-1b01-4ff0-84c8-366445d675ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
